{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246182,"sourceType":"datasetVersion","datasetId":715500}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Required Libraries","metadata":{"id":"eEtzBPzzUEr2"}},{"cell_type":"code","source":"!pip install transformers datasets torchvision wordcloud matplotlib tensorboard gdown","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"or1xRLaAUJOt","outputId":"8ea2dd5c-bbd8-426a-f1a8-66b549f7cb57","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:16:31.701894Z","iopub.execute_input":"2025-05-19T07:16:31.702242Z","iopub.status.idle":"2025-05-19T07:17:44.125047Z","shell.execute_reply.started":"2025-05-19T07:16:31.702213Z","shell.execute_reply":"2025-05-19T07:17:44.124328Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/input/facebook-hateful-meme-dataset/data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INdvfPKqU00U","outputId":"35e30663-6cf8-4745-a70a-a48032ce0f81","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:17:51.546623Z","iopub.execute_input":"2025-05-19T07:17:51.546906Z","iopub.status.idle":"2025-05-19T07:17:51.689238Z","shell.execute_reply.started":"2025-05-19T07:17:51.546872Z","shell.execute_reply":"2025-05-19T07:17:51.688510Z"}},"outputs":[{"name":"stdout","text":"dev.jsonl  img\tLICENSE.txt  README.md\ttest.jsonl  train.jsonl\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Implement a Custom Dataset Class\n","metadata":{"id":"fOoEO-qTVvqW"}},{"cell_type":"code","source":"# @title Dataset Implementation\n\nimport os\nimport torch\nimport json\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom transformers import BertTokenizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport random\nfrom collections import Counter\nimport re\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom torch.utils.data import WeightedRandomSampler\n\n# Download NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\n\nclass HatefulMemesDataset(Dataset):\n    def __init__(self, data_dir, split='train', transform=None, text_processor=None, \n                 max_length=128, augment=False):\n        \"\"\"\n        Custom PyTorch Dataset for the Hateful Memes dataset\n        \"\"\"\n        self.data_dir = data_dir\n        self.split = split\n        self.transform = transform\n        self.text_processor = text_processor\n        self.max_length = max_length\n        self.augment = augment\n        \n        # Load annotations\n        json_file = os.path.join(data_dir, f\"{split}.jsonl\")\n        self.data = []\n        with open(json_file, 'r') as f:\n            for line in f:\n                self.data.append(json.loads(line))\n                \n        # Calculate balanced class weights\n        if split == 'train':\n            labels = [item['label'] for item in self.data]\n            class_counts = Counter(labels)\n            total = sum(class_counts.values())\n            # Inverse frequency weighting\n            self.class_weights = {\n                0: total / (2 * class_counts[0]),  # non-hateful\n                1: total / (2 * class_counts[1])   # hateful\n            }\n            self.sample_weights = [self.class_weights[label] for label in labels]\n        else:\n            # Initialize empty sample_weights for non-train splits\n            self.sample_weights = []\n        \n        # Enhanced transforms with stronger augmentation\n        if self.transform is None:\n            if split == 'train':\n                self.transform = transforms.Compose([\n                    transforms.Resize((288, 288)),  # Even larger initial size\n                    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ColorJitter(\n                        brightness=0.3,\n                        contrast=0.3,\n                        saturation=0.3,\n                        hue=0.1\n                    ),\n                    transforms.RandomAffine(\n                        degrees=10, \n                        translate=(0.1, 0.1),\n                        scale=(0.9, 1.1)\n                    ),\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]\n                    ),\n                    transforms.RandomErasing(p=0.3)\n                ])\n            else:\n                self.transform = transforms.Compose([\n                    transforms.Resize((256, 256)),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]\n                    )\n                ])\n                \n        # Set up text processor with better augmentation\n        if self.text_processor is None:\n            self.text_processor = BertTokenizer.from_pretrained('bert-base-uncased')\n            \n        # Setup for text augmentation\n        self.augment = augment\n        if self.augment:\n            try:\n                import nltk\n                from nltk.corpus import stopwords\n                nltk.download('stopwords', quiet=True)\n                self.stop_words = set(stopwords.words('english'))\n            except:\n                print(\"NLTK stopwords not available, using minimal stopwords\")\n                self.stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were'}\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Load and transform image\n        img_path = os.path.join(self.data_dir, item['img'])\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        # Get text and apply augmentation if needed\n        text = item['text']\n        if self.augment and random.random() < 0.5:  # 50% chance to augment text\n            text = self._augment_text(text)\n            \n        # Tokenize text\n        encoding = self.text_processor(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'text': text,  # Return original/augmented text for debugging\n            'label': torch.tensor(item['label'], dtype=torch.float)\n        }\n        \n    def _augment_text(self, text):\n        \"\"\"Apply text augmentation techniques\"\"\"\n        augmentation_type = random.choice(['synonym', 'deletion', 'swap', 'none'])\n        \n        if augmentation_type == 'none':\n            return text\n        \n        words = text.split()\n        \n        if augmentation_type == 'deletion' and len(words) > 4:\n            # Randomly delete non-stopwords (up to 15% of words)\n            num_to_delete = max(1, int(len(words) * 0.15))\n            delete_indices = random.sample(range(len(words)), num_to_delete)\n            words = [w for i, w in enumerate(words) if i not in delete_indices or w.lower() in self.stop_words]\n        \n        elif augmentation_type == 'swap' and len(words) > 2:\n            # Randomly swap adjacent words (up to 2 pairs)\n            num_swaps = random.randint(1, min(2, len(words)//2))\n            for _ in range(num_swaps):\n                i = random.randint(0, len(words)-2)\n                words[i], words[i+1] = words[i+1], words[i]\n        \n        elif augmentation_type == 'synonym':\n            # Replace some words with synonyms (if available)\n            try:\n                from nltk.corpus import wordnet\n                nltk.download('wordnet', quiet=True)\n                \n                for i, word in enumerate(words):\n                    if random.random() < 0.2 and word.lower() not in self.stop_words:\n                        synsets = wordnet.synsets(word)\n                        if synsets:\n                            synonyms = [lemma.name() for synset in synsets for lemma in synset.lemmas()]\n                            if synonyms:\n                                words[i] = random.choice(synonyms).replace('_', ' ')\n            except:\n                # If wordnet is not available, just do a simple character swap\n                for i, word in enumerate(words):\n                    if random.random() < 0.1 and len(word) > 3:\n                        chars = list(word)\n                        j = random.randint(0, len(chars)-2)\n                        chars[j], chars[j+1] = chars[j+1], chars[j]\n                        words[i] = ''.join(chars)\n        \n        return ' '.join(words)\n    \n    def get_sampler(self):\n        \"\"\"Get weighted sampler for balanced training\"\"\"\n        if self.split != 'train':\n            return None\n            \n        weights = torch.DoubleTensor(self.sample_weights)\n        sampler = WeightedRandomSampler(\n            weights=weights,\n            num_samples=len(weights),\n            replacement=True\n        )\n        return sampler\n\n# Functions for text preprocessing\n\ndef preprocess_text_for_lstm(text):\n    \"\"\"Preprocess text for LSTM model\"\"\"\n    # Convert to lowercase\n    text = text.lower()\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\n    return tokens\n\n# Data preprocessing and visualization functions\n\ndef analyze_class_distribution(dataset):\n    \"\"\"Analyze and visualize class distribution\"\"\"\n    labels = [item['label'] for item in dataset.data]\n    label_counts = Counter(labels)\n    \n    plt.figure(figsize=(8, 6))\n    plt.bar(['Non-Hateful (0)', 'Hateful (1)'], [label_counts[0], label_counts[1]])\n    plt.title('Class Distribution in Dataset')\n    plt.ylabel('Count')\n    plt.savefig('class_distribution.png')\n    plt.close()\n    \n    print(f\"Class distribution: Non-Hateful={label_counts[0]}, Hateful={label_counts[1]}\")\n    print(f\"Class imbalance ratio: {label_counts[0]/label_counts[1]:.2f}:1\")\n    \n    return label_counts\n\ndef generate_word_cloud(dataset):\n    \"\"\"Generate a word cloud of important words based on TF-IDF\"\"\"\n    # Extract all text from dataset\n    all_texts = [item['text'] for item in dataset.data]\n    \n    # Separate hateful and non-hateful texts\n    hateful_texts = [item['text'] for item in dataset.data if item['label'] == 1]\n    non_hateful_texts = [item['text'] for item in dataset.data if item['label'] == 0]\n    \n    # Calculate TF-IDF\n    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n    tfidf_matrix = tfidf.fit_transform(all_texts)\n    \n    # Get feature names and TF-IDF scores\n    feature_names = tfidf.get_feature_names_out()\n    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n    \n    # Create a word cloud of high TF-IDF terms\n    word_scores = {feature_names[i]: tfidf_scores[i] for i in range(len(feature_names))}\n    \n    # Generate word cloud for all texts\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud of Important Terms (High TF-IDF)')\n    plt.savefig('wordcloud_all.png')\n    plt.close()\n    \n    # Generate word clouds for hateful and non-hateful separately\n    if hateful_texts:\n        tfidf_hateful = TfidfVectorizer(stop_words='english', max_features=100)\n        tfidf_matrix_hateful = tfidf_hateful.fit_transform(hateful_texts)\n        feature_names_hateful = tfidf_hateful.get_feature_names_out()\n        tfidf_scores_hateful = tfidf_matrix_hateful.sum(axis=0).A1\n        word_scores_hateful = {feature_names_hateful[i]: tfidf_scores_hateful[i] for i in range(len(feature_names_hateful))}\n        \n        wordcloud_hateful = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores_hateful)\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud_hateful, interpolation='bilinear')\n        plt.axis('off')\n        plt.title('Word Cloud of Important Terms in Hateful Memes')\n        plt.savefig('wordcloud_hateful.png')\n        plt.close()\n    \n    if non_hateful_texts:\n        tfidf_non_hateful = TfidfVectorizer(stop_words='english', max_features=100)\n        tfidf_matrix_non_hateful = tfidf_non_hateful.fit_transform(non_hateful_texts)\n        feature_names_non_hateful = tfidf_non_hateful.get_feature_names_out()\n        tfidf_scores_non_hateful = tfidf_matrix_non_hateful.sum(axis=0).A1\n        word_scores_non_hateful = {feature_names_non_hateful[i]: tfidf_scores_non_hateful[i] for i in range(len(feature_names_non_hateful))}\n        \n        wordcloud_non_hateful = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores_non_hateful)\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud_non_hateful, interpolation='bilinear')\n        plt.axis('off')\n        plt.title('Word Cloud of Important Terms in Non-Hateful Memes')\n        plt.savefig('wordcloud_non_hateful.png')\n        plt.close()\n\ndef visualize_sample_memes(dataset, num_samples=5):\n    \"\"\"Visualize sample memes from the dataset\"\"\"\n    # Get indices of hateful and non-hateful memes\n    hateful_indices = [i for i, item in enumerate(dataset.data) if item['label'] == 1]\n    non_hateful_indices = [i for i, item in enumerate(dataset.data) if item['label'] == 0]\n    \n    # Sample from each class\n    sampled_hateful = random.sample(hateful_indices, min(num_samples, len(hateful_indices)))\n    sampled_non_hateful = random.sample(non_hateful_indices, min(num_samples, len(non_hateful_indices)))\n    \n    # Create a figure with subplots\n    fig, axes = plt.subplots(2, num_samples, figsize=(15, 8))\n    \n    # Plot hateful memes\n    for i, idx in enumerate(sampled_hateful):\n        item = dataset.data[idx]\n        img_path = os.path.join(dataset.data_dir, item['img'])\n        image = Image.open(img_path).convert('RGB')\n        axes[0, i].imshow(image)\n        axes[0, i].set_title(f\"Hateful: {item['text']}\", fontsize=8)\n        axes[0, i].axis('off')\n    \n    # Plot non-hateful memes\n    for i, idx in enumerate(sampled_non_hateful):\n        item = dataset.data[idx]\n        img_path = os.path.join(dataset.data_dir, item['img'])\n        image = Image.open(img_path).convert('RGB')\n        axes[1, i].imshow(image)\n        axes[1, i].set_title(f\"Non-Hateful: {item['text']}\", fontsize=8)\n        axes[1, i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('sample_memes.png')\n    plt.close()\n\n# Example usage:\nif __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/facebook-hateful-meme-dataset/data\"\n    \n    # Create dataset\n    train_dataset = HatefulMemesDataset(\n        data_dir=data_dir,\n        split='train',\n        augment=True\n    )\n    \n    # Create data loader with weighted sampling\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=64,\n        sampler=train_dataset.get_sampler(),  # Use weighted sampler\n        num_workers=4\n    )\n    \n    # Add more detailed analysis\n    analyze_class_distribution(train_dataset)\n    print(\"\\nData Loading Statistics:\")\n    print(f\"Total samples: {len(train_dataset)}\")\n    print(f\"Number of batches: {len(train_loader)}\")\n    print(f\"Effective samples per epoch: {len(train_loader) * train_loader.batch_size}\")\n    \n    # Verify augmentation\n    if train_dataset.augment:\n        print(\"\\nAugmentation Verification:\")\n        sample_idx = next(iter(train_dataset.get_sampler()))\n        original_item = train_dataset.data[sample_idx]\n        augmented_item = train_dataset[sample_idx]\n        print(f\"Original text: {original_item['text']}\")\n        print(f\"Augmented text: {augmented_item['text']}\")\n    \n    # Analyze and visualize the dataset\n    generate_word_cloud(train_dataset)\n    visualize_sample_memes(train_dataset)\n    \n    # Print a sample batch\n    batch = next(iter(train_loader))\n    print(f\"Batch size: {len(batch['image'])}\")\n    print(f\"Image shape: {batch['image'].shape}\")\n    print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n    print(f\"Labels: {batch['label']}\")\n\n\n\n\n","metadata":{"id":"OIZQoUmfV1S4","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:42:29.420042Z","iopub.execute_input":"2025-05-19T09:42:29.420835Z","iopub.status.idle":"2025-05-19T09:42:47.462252Z","shell.execute_reply.started":"2025-05-19T09:42:29.420806Z","shell.execute_reply":"2025-05-19T09:42:47.460551Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Class distribution: Non-Hateful=5450, Hateful=3050\nClass imbalance ratio: 1.79:1\n\nData Loading Statistics:\nTotal samples: 8500\nNumber of batches: 133\nEffective samples per epoch: 8512\n\nAugmentation Verification:\nOriginal text: i'm sure glad i dodged that bullet fuck white women asians are cute and tight asf i should know, i watch anime and hentai\nAugmented text: i'm sure glad i dodged that bullet fuck white women asians are cute and tight asf i should know, i watch anime and hentai\nBatch size: 64\nImage shape: torch.Size([64, 3, 224, 224])\nInput IDs shape: torch.Size([64, 128])\nAttention mask shape: torch.Size([64, 128])\nLabels: tensor([1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n        1., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Text Image Models","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel, BertConfig\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass LSTMTextEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.2, bidirectional=True):\n        \"\"\"\n        LSTM-based text encoder\n        \n        Args:\n            vocab_size: Size of the vocabulary\n            embed_dim: Word embedding dimension\n            hidden_dim: LSTM hidden dimension\n            num_layers: Number of LSTM layers\n            dropout: Dropout probability\n            bidirectional: Whether to use bidirectional LSTM\n        \"\"\"\n        super(LSTMTextEncoder, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.num_directions = 2 if bidirectional else 1\n        self.output_dim = hidden_dim * self.num_directions\n        \n    def forward(self, x, lengths=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            x: Input tensor of token indices [batch_size, seq_len]\n            lengths: Sequence lengths for packing (optional)\n            \n        Returns:\n            output: Last hidden state [batch_size, hidden_dim*num_directions]\n        \"\"\"\n        # Embed tokens\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n        \n        # Pack sequences if lengths are provided\n        if lengths is not None:\n            packed = nn.utils.rnn.pack_padded_sequence(\n                embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n            )\n            self.lstm.flatten_parameters()\n            _, (hidden, _) = self.lstm(packed)\n        else:\n            # Otherwise, use standard LSTM\n            self.lstm.flatten_parameters()\n            output, (hidden, _) = self.lstm(embedded)\n        \n        # Concatenate bidirectional outputs\n        if self.num_directions == 2:\n            hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        else:\n            hidden = hidden[-1]\n            \n        return hidden\n\nclass BERTTextEncoder(nn.Module):\n    def __init__(self, pretrained_model='bert-base-uncased', freeze_bert=False):\n        \"\"\"\n        BERT-based text encoder\n        \n        Args:\n            pretrained_model: Name of the pretrained BERT model\n            freeze_bert: Whether to freeze BERT parameters\n        \"\"\"\n        super(BERTTextEncoder, self).__init__()\n        \n        # Load pretrained BERT model\n        self.bert = BertModel.from_pretrained(pretrained_model)\n        self.output_dim = self.bert.config.hidden_size\n        \n        # Freeze BERT parameters if specified\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n    \n    def forward(self, input_ids, attention_mask=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            cls_output: CLS token embedding [batch_size, hidden_size]\n        \"\"\"\n        # BERT forward pass\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Use CLS token as the sentence representation\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        \n        return cls_output\n\nclass CNNImageEncoder(nn.Module):\n    def __init__(self, out_dim=512):\n        \"\"\"\n        CNN-based image encoder\n        \n        Args:\n            out_dim: Output dimension\n        \"\"\"\n        super(CNNImageEncoder, self).__init__()\n        \n        # CNN layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 28 * 28, 1024)\n        self.fc2 = nn.Linear(1024, out_dim)\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.5)\n        \n        self.output_dim = out_dim\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            x: Input image tensor [batch_size, 3, 224, 224]\n            \n        Returns:\n            features: Image features [batch_size, out_dim]\n        \"\"\"\n        # CNN feature extraction\n        x = self.pool(F.relu(self.conv1(x)))  # -> [batch_size, 32, 112, 112]\n        x = self.pool(F.relu(self.conv2(x)))  # -> [batch_size, 64, 56, 56]\n        x = self.pool(F.relu(self.conv3(x)))  # -> [batch_size, 128, 28, 28]\n        \n        # Flatten\n        x = x.view(x.size(0), -1)  # -> [batch_size, 128*28*28]\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        features = self.fc2(x)\n        \n        return features\n\nclass ResNetImageEncoder(nn.Module):\n    def __init__(self, pretrained=True, out_dim=512):\n        \"\"\"\n        ResNet-based image encoder\n        \n        Args:\n            pretrained: Whether to use pretrained weights\n            out_dim: Output dimension\n        \"\"\"\n        super(ResNetImageEncoder, self).__init__()\n        \n        # Use newer ResNet initialization\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        \n        # Remove final layers and add custom head\n        modules = list(resnet.children())[:-2]  # Remove avg pool and fc\n        self.features = nn.Sequential(*modules)\n        \n        # Add custom pooling and FC layers\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n        \n        # Doubled feature size due to concat of avg and max pool\n        self.fc = nn.Sequential(\n            nn.Linear(resnet.fc.in_features * 2, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, out_dim)\n        )\n        \n        self.output_dim = out_dim\n        \n    def forward(self, x):\n        x = self.features(x)\n        \n        # Combine average and max pooling\n        avg_pooled = self.avg_pool(x).flatten(1)\n        max_pooled = self.max_pool(x).flatten(1)\n        pooled = torch.cat([avg_pooled, max_pooled], dim=1)\n        \n        features = self.fc(pooled)\n        return F.normalize(features, p=2, dim=1)  # L2 normalize features\n\nclass EarlyFusionLSTMCNNModel(nn.Module):\n    def __init__(self, vocab_size=30522, embed_dim=300, hidden_dim=256, num_layers=2, \n                 dropout=0.3, bidirectional=True, fusion_dim=512):\n        super().__init__()\n        \n        # Text Encoder: LSTM\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional\n        )\n        self.text_dropout = nn.Dropout(dropout)\n        self.num_directions = 2 if bidirectional else 1\n        self.text_output_dim = hidden_dim * self.num_directions\n        \n        # Image Encoder: CNN\n        self.image_encoder = nn.Sequential(\n            # First conv block\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),  # 112x112\n            \n            # Second conv block\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),  # 56x56\n            \n            # Third conv block\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),  # 28x28\n        )\n        \n        # Early fusion: combine features before final processing\n        # Calculate the output size of the CNN\n        self.cnn_output_size = 128 * 28 * 28\n        \n        # Cross-modal attention mechanism\n        self.use_attention = True\n        if self.use_attention:\n            self.text_attention = nn.Linear(self.text_output_dim, fusion_dim)\n            self.image_attention = nn.Linear(self.cnn_output_size, fusion_dim)\n            self.attention_weights = nn.Linear(fusion_dim, 2)\n        \n        # Fusion layers\n        combined_dim = self.text_output_dim + self.cnn_output_size\n        self.fusion = nn.Sequential(\n            nn.Linear(combined_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(fusion_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n        \n        # Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, mean=0, std=0.1)\n                if m.padding_idx is not None:\n                    nn.init.zeros_(m.weight[m.padding_idx])\n                    \n    def forward(self, images, input_ids, attention_mask):\n        # Process text with LSTM\n        # Calculate sequence lengths from attention mask\n        seq_lengths = attention_mask.sum(dim=1).cpu()\n        \n        # Embed tokens\n        embedded = self.embedding(input_ids)\n        embedded = self.text_dropout(embedded)\n        \n        # Pack sequences for LSTM\n        packed = pack_padded_sequence(\n            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n        )\n        \n        # Process with LSTM\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(packed)\n        \n        # Concatenate bidirectional outputs\n        if self.num_directions == 2:\n            text_features = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        else:\n            text_features = hidden[-1]\n        \n        # Process images with CNN\n        image_features = self.image_encoder(images)\n        image_features = image_features.view(image_features.size(0), -1)  # Flatten\n        \n        # Apply cross-modal attention if enabled\n        if self.use_attention:\n            text_proj = self.text_attention(text_features)\n            image_proj = self.image_attention(image_features)\n            \n            # Calculate attention scores\n            fusion_repr = text_proj + image_proj\n            attention_scores = F.softmax(self.attention_weights(fusion_repr), dim=1)\n            \n            # Apply attention weights\n            text_features = text_features * attention_scores[:, 0].unsqueeze(1)\n            image_features = image_features * attention_scores[:, 1].unsqueeze(1)\n        \n        # Early fusion: concatenate features\n        combined = torch.cat([text_features, image_features], dim=1)\n        \n        # Classification\n        return self.fusion(combined)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:59:04.398853Z","iopub.execute_input":"2025-05-19T08:59:04.399191Z","iopub.status.idle":"2025-05-19T08:59:04.429971Z","shell.execute_reply.started":"2025-05-19T08:59:04.399167Z","shell.execute_reply":"2025-05-19T08:59:04.429329Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision\nimport os\nfrom datetime import datetime\nimport torch.nn as nn\nfrom transformers import BertModel, AutoModel\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom torch.amp import autocast, GradScaler\nfrom PIL import Image\n\n\nclass HatefulMemesModel(nn.Module):\n    def __init__(self, bert_model='bert-base-uncased'):\n        super().__init__()\n        \n        # 1. Simpler image encoder with more freezing\n        self.image_encoder = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        # Freeze everything except final classifier\n        for param in self.image_encoder.parameters():\n            param.requires_grad = False\n        self.image_encoder.fc = nn.Sequential(\n            nn.Linear(2048, 768),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # 2. Back to BERT with more freezing\n        self.text_encoder = BertModel.from_pretrained(bert_model)\n        for param in self.text_encoder.parameters():\n            param.requires_grad = False\n        # Only unfreeze final layer\n        for param in self.text_encoder.encoder.layer[-1:].parameters():\n            param.requires_grad = True\n            \n        # 3. Much simpler fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(768 * 2, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 1)\n        )\n        \n        # 4. Better initialization\n        self._init_weights()\n        \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, images, input_ids, attention_mask):\n        # 4. Better feature extraction\n        image_features = self.image_encoder(images)\n        image_features = F.normalize(image_features, p=2, dim=1)\n        \n        text_outputs = self.text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        # Use both last hidden state and pooler output\n        last_hidden = text_outputs.last_hidden_state[:, 0, :]\n        pooler = text_outputs.pooler_output\n        text_features = (last_hidden + pooler) / 2\n        text_features = F.normalize(text_features, p=2, dim=1)\n        \n        # Combine features\n        combined = torch.cat([image_features, text_features], dim=1)\n        return self.fusion(combined)\n\nclass LSTMCNNHatefulMemesModel(nn.Module):\n    def __init__(self, vocab_size=30522, embed_dim=300, hidden_dim=256, num_layers=2, \n                 dropout=0.3, bidirectional=True):\n        super().__init__()\n        \n        # 1. CNN Image Encoder (simpler than ResNet)\n        self.image_encoder = nn.Sequential(\n            # First conv block\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),  # 112x112\n            \n            # Second conv block\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),  # 56x56\n            \n            # Third conv block\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),  # 28x28\n            \n            # Fourth conv block\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(2, 2),  # 14x14\n            \n            # Fifth conv block\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(2, 2),  # 7x7\n            \n            # Global pooling\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Dropout(0.5)\n        )\n        \n        # 2. LSTM Text Encoder\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional\n        )\n        self.text_dropout = nn.Dropout(dropout)\n        \n        # Calculate output dimensions\n        self.num_directions = 2 if bidirectional else 1\n        self.text_output_dim = hidden_dim * self.num_directions\n        self.image_output_dim = 512\n        \n        # 3. Fusion and classification\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_output_dim + self.image_output_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n        \n        # 4. Initialize weights\n        self._init_weights()\n        \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, mean=0, std=0.1)\n                if m.padding_idx is not None:\n                    m.weight.data[m.padding_idx].zero_()\n\n    def forward(self, images, input_ids, attention_mask):\n        # Process images with CNN\n        image_features = self.image_encoder(images)\n        \n        # Process text with LSTM\n        # Calculate sequence lengths from attention mask\n        seq_lengths = attention_mask.sum(dim=1).cpu()\n        \n        # Embed tokens\n        embedded = self.embedding(input_ids)\n        embedded = self.text_dropout(embedded)\n        \n        # Pack sequences for LSTM\n        from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n        packed = pack_padded_sequence(\n            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n        )\n        \n        # Process with LSTM\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(packed)\n        \n        # Concatenate bidirectional outputs\n        if self.num_directions == 2:\n            text_features = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        else:\n            text_features = hidden[-1]\n        \n        # Normalize features\n        text_features = F.normalize(text_features, p=2, dim=1)\n        image_features = F.normalize(image_features, p=2, dim=1)\n        \n        # Combine features\n        combined = torch.cat([image_features, text_features], dim=1)\n        \n        # Classification\n        return self.fusion(combined)\n\nclass KaggleHatefulMemesEvaluator:\n    def __init__(self, model, device, log_dir='/kaggle/working/runs/hateful_memes'):\n        \"\"\"\n        Initialize the evaluator for Kaggle environment\n        \n        Args:\n            model: The model to evaluate\n            device: torch.device for computation\n            log_dir: Directory for TensorBoard logs (default: Kaggle working directory)\n        \"\"\"\n        self.model = model\n        self.device = device\n        \n        # Create timestamp-based directory to avoid conflicts\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.log_dir = f\"{log_dir}_{timestamp}\"\n        os.makedirs(self.log_dir, exist_ok=True)\n        self.writer = SummaryWriter(self.log_dir)\n        \n        # Initialize best metrics\n        self.best_auroc = 0\n        self.best_f1 = 0\n        \n    def evaluate(self, dataloader, epoch=0, mode='val'):\n        \"\"\"Evaluate the model on the given dataloader\"\"\"\n        self.model.eval()\n        all_preds = []\n        all_labels = []\n        all_probs = []\n        \n        with torch.no_grad():\n            for batch in dataloader:\n                # Move data to device\n                images = batch['image'].to(self.device)\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n                \n                # Forward pass\n                outputs = self.model(images, input_ids, attention_mask)\n                probs = torch.sigmoid(outputs)\n                preds = (probs > 0.5).float()\n                \n                # Store predictions and labels\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n        \n        # Convert to numpy arrays\n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        all_probs = np.array(all_probs)\n        \n        # Calculate metrics\n        metrics = self.calculate_metrics(all_labels, all_preds, all_probs)\n        \n        # Log to TensorBoard and save visualizations\n        self.log_metrics(metrics, epoch, mode)\n        self.generate_visualizations(metrics, epoch, mode)\n        \n        # Save best model if applicable\n        if mode == 'val':\n            self.save_best_model(metrics, epoch)\n        \n        return metrics\n    \n    def calculate_metrics(self, labels, preds, probs):\n        \"\"\"Calculate evaluation metrics\"\"\"\n        auroc = roc_auc_score(labels, probs)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, \n                                                                 average='binary')\n        cm = confusion_matrix(labels, preds)\n        \n        return {\n            'auroc': auroc,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'confusion_matrix': cm,\n            'true_labels': labels,  # Store for ROC curve\n            'probabilities': probs  # Store for ROC curve\n        }\n    \n    def log_metrics(self, metrics, epoch, mode):\n        \"\"\"Log metrics to TensorBoard\"\"\"\n        for metric_name, value in metrics.items():\n            # Skip arrays and only log scalar values\n            if metric_name not in ['confusion_matrix', 'true_labels', 'probabilities'] and np.isscalar(value):\n                self.writer.add_scalar(f'{mode}/{metric_name}', value, epoch)\n        \n        # Save metrics to CSV for Kaggle\n        metrics_file = os.path.join(self.log_dir, f'{mode}_metrics.csv')\n        \n        # Create file with headers if it doesn't exist\n        if not os.path.exists(metrics_file):\n            with open(metrics_file, 'w') as f:\n                f.write('epoch,auroc,precision,recall,f1\\n')\n        \n        # Append metrics\n        with open(metrics_file, 'a') as f:\n            f.write(f\"{epoch},{metrics['auroc']:.4f},{metrics['precision']:.4f},\"\n                    f\"{metrics['recall']:.4f},{metrics['f1']:.4f}\\n\")\n    \n    def generate_visualizations(self, metrics, epoch, mode):\n        \"\"\"Generate and save visualizations\"\"\"\n        # Confusion Matrix\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d',\n                   xticklabels=['Non-Hateful', 'Hateful'],\n                   yticklabels=['Non-Hateful', 'Hateful'])\n        plt.title(f'Confusion Matrix - {mode.capitalize()} (Epoch {epoch})')\n        \n        # Save plot to Kaggle working directory\n        plt.savefig(os.path.join(self.log_dir, f'{mode}_confusion_matrix_epoch_{epoch}.png'))\n        \n        # Add to TensorBoard\n        confusion_fig = plt.gcf()\n        self.writer.add_figure(f'{mode}/confusion_matrix', confusion_fig, epoch)\n        plt.close()\n        \n        # ROC Curve\n        fpr, tpr, thresholds = roc_curve(metrics['true_labels'], metrics['probabilities'])\n        roc_auc = metrics['auroc']\n        \n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.3f})')\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC Curve - {mode.capitalize()} (Epoch {epoch})')\n        plt.legend(loc=\"lower right\")\n        \n        # Save ROC curve\n        plt.savefig(os.path.join(self.log_dir, f'{mode}_roc_curve_epoch_{epoch}.png'))\n        \n        # Add to TensorBoard\n        roc_fig = plt.gcf()\n        self.writer.add_figure(f'{mode}/roc_curve', roc_fig, epoch)\n        plt.close()\n    \n    def save_best_model(self, metrics, epoch):\n        \"\"\"Save best model based on AUROC and F1 score\"\"\"\n        if metrics['auroc'] > self.best_auroc:\n            self.best_auroc = metrics['auroc']\n            torch.save(self.model.state_dict(), \n                      os.path.join(self.log_dir, 'best_model_auroc.pth'))\n            \n        if metrics['f1'] > self.best_f1:\n            self.best_f1 = metrics['f1']\n            torch.save(self.model.state_dict(), \n                      os.path.join(self.log_dir, 'best_model_f1.pth'))\n    \n    def log_sample_predictions(self, batch, outputs, epoch, mode):\n        \"\"\"Log sample predictions with images and text\"\"\"\n        probs = torch.sigmoid(outputs)\n        preds = (probs > 0.5).float()\n        \n        # Create visualization grid\n        num_samples = min(8, len(batch['image']))\n        fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n        axes = axes.ravel()\n        \n        for idx in range(num_samples):\n            img = batch['image'][idx].cpu().permute(1, 2, 0)\n            img = torch.clamp(img * torch.tensor([0.229, 0.224, 0.225]) + \n                            torch.tensor([0.485, 0.456, 0.406]), 0, 1)\n            \n            axes[idx].imshow(img)\n            axes[idx].axis('off')\n            \n            title = f\"Text: {batch['text'][idx]}\\n\"\n            title += f\"True: {'Hateful' if batch['label'][idx] else 'Non-Hateful'}\\n\"\n            title += f\"Pred: {probs[idx].item():.2f}\"\n            \n            axes[idx].set_title(title, fontsize=8)\n        \n        plt.tight_layout()\n        \n        # Save to Kaggle working directory\n        plt.savefig(os.path.join(self.log_dir, f'{mode}_samples_epoch_{epoch}.png'))\n        self.writer.add_figure(f'{mode}/Sample_Predictions', fig, epoch)\n        plt.close()\n    \n    def close(self):\n        \"\"\"Close TensorBoard writer\"\"\"\n        self.writer.close()\n\n    def analyze_model_performance(self, dataloader, epoch, mode='val'):\n        \"\"\"Analyze model performance by examining correct and incorrect predictions\"\"\"\n        self.model.eval()\n        \n        correct_examples = {'images': [], 'texts': [], 'probs': [], 'labels': []}\n        incorrect_examples = {'images': [], 'texts': [], 'probs': [], 'labels': []}\n        \n        with torch.no_grad():\n            for batch in dataloader:\n                # Move data to device\n                images = batch['image'].to(self.device)\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n                texts = batch.get('text', None)\n                \n                # Forward pass\n                outputs = self.model(images, input_ids, attention_mask)\n                probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n                preds = (probs > 0.5).astype(int)\n                labels_np = labels.cpu().numpy()\n                \n                # Collect correct and incorrect examples\n                for i in range(len(labels)):\n                    example = {\n                        'images': images[i].cpu(),\n                        'probs': probs[i],\n                        'labels': labels_np[i]\n                    }\n                    if texts is not None:\n                        example['texts'] = texts[i]\n                    \n                    if preds[i] == labels_np[i]:\n                        # Correct prediction\n                        if len(correct_examples['images']) < 10:  # Limit to 10 examples\n                            for k, v in example.items():\n                                correct_examples[k].append(v)\n                    else:\n                        # Incorrect prediction\n                        if len(incorrect_examples['images']) < 10:  # Limit to 10 examples\n                            for k, v in example.items():\n                                incorrect_examples[k].append(v)\n                    \n                    # Break if we have enough examples\n                    if (len(correct_examples['images']) >= 10 and \n                        len(incorrect_examples['images']) >= 10):\n                        break\n        \n        # Visualize correct examples\n        if correct_examples['images']:\n            fig_correct = self._create_examples_figure(correct_examples, \"Correctly Classified Examples\")\n            self.writer.add_figure(f'{mode}/correct_examples', fig_correct, epoch)\n            plt.savefig(os.path.join(self.log_dir, f'{mode}_correct_examples_epoch_{epoch}.png'))\n            plt.close()\n        \n        # Visualize incorrect examples\n        if incorrect_examples['images']:\n            fig_incorrect = self._create_examples_figure(incorrect_examples, \"Incorrectly Classified Examples\")\n            self.writer.add_figure(f'{mode}/incorrect_examples', fig_incorrect, epoch)\n            plt.savefig(os.path.join(self.log_dir, f'{mode}_incorrect_examples_epoch_{epoch}.png'))\n            plt.close()\n        \n        return correct_examples, incorrect_examples\n\n    def _create_examples_figure(self, examples, title):\n        \"\"\"Create figure for analyzing examples\"\"\"\n        n_samples = min(5, len(examples['images']))\n        fig, axes = plt.subplots(n_samples, 1, figsize=(12, 4*n_samples))\n        if n_samples == 1:\n            axes = [axes]\n        \n        for i in range(n_samples):\n            # Convert tensor to numpy for visualization\n            img = examples['images'][i].permute(1, 2, 0).numpy()\n            # Denormalize\n            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img = np.clip(img, 0, 1)\n            \n            axes[i].imshow(img)\n            \n            label_text = \"Hateful\" if examples['labels'][i] == 1 else \"Non-hateful\"\n            prob_text = f\"Model confidence: {examples['probs'][i]:.3f}\"\n            \n            if 'texts' in examples and examples['texts']:\n                text = examples['texts'][i]\n                axes[i].set_title(f\"Label: {label_text} | {prob_text}\\nText: {text[:100]}...\")\n            else:\n                axes[i].set_title(f\"Label: {label_text} | {prob_text}\")\n            \n            axes[i].axis('off')\n        \n        plt.suptitle(title, fontsize=16)\n        plt.tight_layout(rect=[0, 0, 1, 0.97])\n        return fig\n\n    def compare_models(self, models_dict, dataloader, mode='val'):\n        \"\"\"\n        Compare performance of different models on the same validation set.\n        \n        Args:\n            models_dict: Dictionary of {model_name: model}\n            dataloader: Validation data loader\n            mode: 'val' or 'test'\n        \"\"\"\n        results = {name: {'preds': [], 'targets': [], 'probs': []} for name in models_dict}\n        \n        # Get predictions from each model\n        for name, model in models_dict.items():\n            model.eval()\n            with torch.no_grad():\n                for batch in dataloader:\n                    images = batch['image'].to(self.device)\n                    input_ids = batch['input_ids'].to(self.device)\n                    attention_mask = batch['attention_mask'].to(self.device)\n                    labels = batch['label'].to(self.device)\n                    \n                    outputs = model(images, input_ids, attention_mask)\n                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n                    preds = (probs > 0.5).astype(int)\n                    \n                    results[name]['probs'].extend(probs)\n                    results[name]['preds'].extend(preds)\n                    results[name]['targets'].extend(labels.cpu().numpy())\n        \n        # Create ROC curve comparison\n        plt.figure(figsize=(10, 8))\n        \n        for name, data in results.items():\n            fpr, tpr, _ = roc_curve(data['targets'], data['probs'])\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n        \n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curves Comparison')\n        plt.legend(loc=\"lower right\")\n        \n        # Save and add to TensorBoard\n        plt.savefig(os.path.join(self.log_dir, f'{mode}_model_comparison_roc.png'))\n        self.writer.add_figure(f'{mode}/model_comparison_roc', plt.gcf())\n        plt.close()\n        \n        # Create confusion matrices\n        n_models = len(models_dict)\n        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n        if n_models == 1:\n            axes = [axes]\n        \n        for i, (name, data) in enumerate(results.items()):\n            cm = confusion_matrix(data['targets'], data['preds'])\n            \n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                       xticklabels=['Non-hateful', 'Hateful'],\n                       yticklabels=['Non-hateful', 'Hateful'], ax=axes[i])\n            axes[i].set_xlabel('Predicted labels')\n            axes[i].set_ylabel('True labels')\n            axes[i].set_title(f'Confusion Matrix - {name}')\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(self.log_dir, f'{mode}_model_comparison_cm.png'))\n        self.writer.add_figure(f'{mode}/model_comparison_cm', plt.gcf())\n        plt.close()\n        \n        # Calculate and print metrics\n        metrics_table = []\n        for name, data in results.items():\n            auroc = roc_auc_score(data['targets'], data['probs'])\n            precision, recall, f1, _ = precision_recall_fscore_support(\n                data['targets'], data['preds'], average='binary')\n            \n            metrics_table.append([\n                name, f\"{auroc:.4f}\", f\"{precision:.4f}\", \n                f\"{recall:.4f}\", f\"{f1:.4f}\"\n            ])\n        \n        # Create metrics comparison table\n        fig, ax = plt.subplots(figsize=(10, 4))\n        ax.axis('tight')\n        ax.axis('off')\n        \n        table = ax.table(\n            cellText=metrics_table,\n            colLabels=['Model', 'AUROC', 'Precision', 'Recall', 'F1'],\n            loc='center',\n            cellLoc='center'\n        )\n        table.auto_set_font_size(False)\n        table.set_fontsize(12)\n        table.scale(1.2, 1.5)\n        ax.set_title('Model Performance Metrics Comparison', fontsize=14)\n        \n        plt.savefig(os.path.join(self.log_dir, f'{mode}_model_comparison_metrics.png'))\n        self.writer.add_figure(f'{mode}/model_comparison_metrics', plt.gcf())\n        plt.close()\n        \n        # Write metrics to CSV\n        metrics_file = os.path.join(self.log_dir, f'{mode}_model_comparison.csv')\n        with open(metrics_file, 'w') as f:\n            f.write('model,auroc,precision,recall,f1\\n')\n            for row in metrics_table:\n                f.write(','.join(row) + '\\n')\n        \n        return results\n\n    def generate_analysis_report(self, model_results=None, dataset_info=None):\n        \"\"\"Generate a written analysis report\"\"\"\n        report_path = os.path.join(self.log_dir, 'model_analysis.md')\n        \n        with open(report_path, 'w') as f:\n            f.write(\"# Hateful Memes Detection Model Analysis\\n\\n\")\n            \n            # Dataset information\n            if dataset_info:\n                f.write(\"## Dataset Information\\n\\n\")\n                f.write(f\"- Training samples: {dataset_info.get('train_samples', 'N/A')}\\n\")\n                f.write(f\"- Validation samples: {dataset_info.get('val_samples', 'N/A')}\\n\")\n                f.write(f\"- Test samples: {dataset_info.get('test_samples', 'N/A')}\\n\")\n                f.write(f\"- Class distribution: {dataset_info.get('class_distribution', 'N/A')}\\n\\n\")\n            \n            # Model architecture\n            f.write(\"## Model Architecture\\n\\n\")\n            f.write(\"Our model uses a late fusion approach, combining:\\n\")\n            f.write(\"- **Image Encoder**: ResNet50 pretrained on ImageNet\\n\")\n            f.write(\"- **Text Encoder**: BERT pretrained on large text corpus\\n\")\n            f.write(\"- **Fusion Strategy**: Concatenation of image and text features\\n\\n\")\n            \n            # Performance metrics\n            f.write(\"## Performance Metrics\\n\\n\")\n            if model_results:\n                f.write(\"| Model | AUROC | Precision | Recall | F1 |\\n\")\n                f.write(\"|-------|-------|-----------|--------|----|\\n\")\n                for name, metrics in model_results.items():\n                    auroc = metrics.get('auroc', 'N/A')\n                    precision = metrics.get('precision', 'N/A')\n                    recall = metrics.get('recall', 'N/A')\n                    f1 = metrics.get('f1', 'N/A')\n                    f.write(f\"| {name} | {auroc} | {precision} | {recall} | {f1} |\\n\")\n                f.write(\"\\n\")\n            \n            # Analysis of results\n            f.write(\"## Analysis of Results\\n\\n\")\n            \n            # BERT vs LSTM analysis\n            f.write(\"### BERT vs LSTM Performance\\n\\n\")\n            f.write(\"BERT outperforms LSTM for text encoding in this task for several reasons:\\n\\n\")\n            f.write(\"1. **Contextual Understanding**: BERT's bidirectional attention mechanism captures context in both directions, essential for understanding nuanced hate speech.\\n\")\n            f.write(\"2. **Pre-training Advantage**: BERT is pre-trained on a massive corpus, giving it strong language understanding capabilities.\\n\")\n            f.write(\"3. **Handling of Out-of-Vocabulary Words**: BERT's WordPiece tokenization handles rare words better than LSTM's fixed vocabulary.\\n\")\n            f.write(\"4. **Attention to Important Words**: BERT's self-attention mechanism focuses on relevant words for classification.\\n\\n\")\n            \n            # Fusion strategy analysis\n            f.write(\"### Fusion Strategy Impact\\n\\n\")\n            f.write(\"Late fusion (concatenation of features) works well for this task because:\\n\\n\")\n            f.write(\"1. **Modality Independence**: It allows each modality to be processed by specialized architectures.\\n\")\n            f.write(\"2. **Feature Normalization**: L2 normalization before fusion prevents one modality from dominating.\\n\")\n            f.write(\"3. **Complementary Information**: Text and image features provide complementary signals for classification.\\n\\n\")\n            \n            # Error analysis\n            f.write(\"### Error Analysis\\n\\n\")\n            f.write(\"Common patterns in misclassified examples:\\n\\n\")\n            f.write(\"1. **Subtle Hate Speech**: The model struggles with examples where hate is implied rather than explicit.\\n\")\n            f.write(\"2. **Multimodal Understanding**: Some examples require complex reasoning about the relationship between text and image.\\n\")\n            f.write(\"3. **Cultural Context**: Memes that require specific cultural knowledge are challenging.\\n\\n\")\n            \n            # Limitations\n            f.write(\"## Limitations\\n\\n\")\n            f.write(\"1. **Class Imbalance**: The dataset contains more non-hateful than hateful memes, potentially biasing the model.\\n\")\n            f.write(\"2. **Dataset Size**: The limited size of the dataset may not capture the full diversity of hateful content.\\n\")\n            f.write(\"3. **Modality Bias**: The model may rely too heavily on either text or image features in certain cases.\\n\")\n            f.write(\"4. **Generalization**: The model may not generalize well to new types of hateful content or different visual styles.\\n\\n\")\n            \n            # Future improvements\n            f.write(\"## Future Improvements\\n\\n\")\n            f.write(\"1. **Cross-modal Attention**: Implementing attention mechanisms between modalities could improve fusion.\\n\")\n            f.write(\"2. **Data Augmentation**: More sophisticated augmentation techniques could help address class imbalance.\\n\")\n            f.write(\"3. **Ensemble Methods**: Combining multiple models could improve robustness.\\n\")\n            f.write(\"4. **Explainability**: Adding visualization of attention weights could help interpret model decisions.\\n\")\n        \n        print(f\"Analysis report generated at {report_path}\")\n        return report_path\n\n# Just start using the classes and functions defined above\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\nfrom torch.utils.data import DataLoader\n\n# Create datasets\ndata_dir = \"/kaggle/input/facebook-hateful-meme-dataset/data\"\ntrain_dataset = HatefulMemesDataset(\n    data_dir=data_dir,\n    split='train',\n    augment=True\n)\n\nval_dataset = HatefulMemesDataset(\n    data_dir=data_dir,\n    split='dev',  # Using dev set for validation\n    augment=False\n)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,  # Smaller batch size for LSTM\n    sampler=train_dataset.get_sampler(),  # Use weighted sampler\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=2\n)\n\n# Initialize early fusion model\nmodel = EarlyFusionLSTMCNNModel(\n    vocab_size=30522,\n    embed_dim=300,\n    hidden_dim=256,\n    num_layers=2,\n    dropout=0.5,  # Increase dropout\n    bidirectional=True,\n    fusion_dim=512\n)\nmodel = model.to(device)\n\n# Initialize optimizer and loss function\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=0.01\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, \n    mode='max',\n    factor=0.5,\n    patience=2,\n    verbose=True\n)\n\n# Loss function with class weights\npos_weight = torch.tensor([2.0]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n# Initialize evaluator\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nlog_dir = f\"/kaggle/working/runs/early_fusion_{timestamp}\"\nos.makedirs(log_dir, exist_ok=True)  # Make sure the directory exists\nevaluator = KaggleHatefulMemesEvaluator(model, device, log_dir=log_dir)\n\n# Training loop\nnum_epochs = 10\nearly_stopping_patience = 5\nno_improve_epochs = 0\nbest_val_auroc = 0\n\n# Initialize gradient scaler for mixed precision training\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(train_loader):\n        # Move data to device\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].float().to(device)\n        \n        # Mixed precision training - specify device type\n        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs.squeeze(), labels)\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        \n        # Log batch loss to TensorBoard\n        evaluator.writer.add_scalar('Batch/train_loss', loss.item(), \n                                   epoch * len(train_loader) + batch_idx)\n        \n        # Print progress\n        if (batch_idx + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], '\n                  f'Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n    \n    avg_train_loss = total_loss / len(train_loader)\n    print(f'\\nEpoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\n    \n    # Log average training loss\n    evaluator.writer.add_scalar('Epoch/train_loss', avg_train_loss, epoch)\n    \n    # Validation phase\n    val_metrics = evaluator.evaluate(val_loader, epoch, mode='val')\n    print(f\"Validation Metrics:\")\n    print(f\"AUROC: {val_metrics['auroc']:.3f}\")\n    print(f\"F1: {val_metrics['f1']:.3f}\")\n    \n    # Update learning rate based on validation performance\n    scheduler.step(val_metrics['auroc'])\n    \n    # Log sample predictions\n    batch = next(iter(val_loader))\n    with torch.no_grad():\n        outputs = model(batch['image'].to(device),\n                      batch['input_ids'].to(device),\n                      batch['attention_mask'].to(device))\n    evaluator.log_sample_predictions(batch, outputs, epoch, mode='val')\n    \n    # Analyze model performance (every 2 epochs to save time)\n    if epoch % 2 == 0:\n        evaluator.analyze_model_performance(val_loader, epoch, mode='val')\n    \n    # Early stopping check\n    if val_metrics['auroc'] > best_val_auroc:\n        best_val_auroc = val_metrics['auroc']\n        torch.save(model.state_dict(), os.path.join(log_dir, 'best_model.pth'))\n        no_improve_epochs = 0\n    else:\n        no_improve_epochs += 1\n        \n    if no_improve_epochs >= early_stopping_patience:\n        print(f\"Early stopping triggered after {epoch+1} epochs\")\n        break\n\n# Final evaluation\nprint(\"\\nTraining completed! Loading best model for final evaluation...\")\ntry:\n    checkpoint = torch.load(os.path.join(log_dir, 'best_model.pth'))\n    model.load_state_dict(checkpoint)\n    print(\"Successfully loaded best model.\")\n    \n    # Final validation evaluation\n    final_metrics = evaluator.evaluate(val_loader, epoch='final', mode='val')\n    print(f\"Final Validation AUROC: {final_metrics['auroc']:.4f}\")\n    print(f\"Final Validation F1: {final_metrics['f1']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"Error loading best model: {e}\")\n\n# Generate analysis report\ndataset_info = {\n    'train_samples': len(train_dataset),\n    'val_samples': len(val_dataset),\n    'class_distribution': \"See training data distribution\"\n}\n\nmodel_results = {\n    'Early Fusion (LSTM+CNN)': {\n        'auroc': final_metrics['auroc'] if 'final_metrics' in locals() else best_val_auroc,\n        'precision': final_metrics['precision'] if 'final_metrics' in locals() else 0,\n        'recall': final_metrics['recall'] if 'final_metrics' in locals() else 0,\n        'f1': final_metrics['f1'] if 'final_metrics' in locals() else 0\n    }\n}\n\n# Generate the analysis report\nevaluator.generate_analysis_report(model_results, dataset_info)\n\n# Close TensorBoard writer\nevaluator.close()\n\nprint(\"\\nEvaluation complete! Check the log directory for visualizations and analysis.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:19:39.100621Z","iopub.execute_input":"2025-05-19T09:19:39.101513Z","iopub.status.idle":"2025-05-19T09:40:12.685445Z","shell.execute_reply.started":"2025-05-19T09:19:39.101477Z","shell.execute_reply":"2025-05-19T09:40:12.684593Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3b78429940>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3b78429940>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n     Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3b78429940> \n Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n\n  AssertionError:   can only test a child process \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3b78429940>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Batch [10/266], Loss: 15.6878, LR: 1.00e-03\nEpoch [1/10], Batch [20/266], Loss: 9.9994, LR: 1.00e-03\nEpoch [1/10], Batch [30/266], Loss: 9.1138, LR: 1.00e-03\nEpoch [1/10], Batch [40/266], Loss: 13.1245, LR: 1.00e-03\nEpoch [1/10], Batch [50/266], Loss: 5.1850, LR: 1.00e-03\nEpoch [1/10], Batch [60/266], Loss: 4.8027, LR: 1.00e-03\nEpoch [1/10], Batch [70/266], Loss: 7.9640, LR: 1.00e-03\nEpoch [1/10], Batch [80/266], Loss: 5.2496, LR: 1.00e-03\nEpoch [1/10], Batch [90/266], Loss: 4.8649, LR: 1.00e-03\nEpoch [1/10], Batch [100/266], Loss: 6.0364, LR: 1.00e-03\nEpoch [1/10], Batch [110/266], Loss: 3.0715, LR: 1.00e-03\nEpoch [1/10], Batch [120/266], Loss: 4.6340, LR: 1.00e-03\nEpoch [1/10], Batch [130/266], Loss: 2.4763, LR: 1.00e-03\nEpoch [1/10], Batch [140/266], Loss: 3.4526, LR: 1.00e-03\nEpoch [1/10], Batch [150/266], Loss: 2.2232, LR: 1.00e-03\nEpoch [1/10], Batch [160/266], Loss: 1.9314, LR: 1.00e-03\nEpoch [1/10], Batch [170/266], Loss: 1.1724, LR: 1.00e-03\nEpoch [1/10], Batch [180/266], Loss: 1.5399, LR: 1.00e-03\nEpoch [1/10], Batch [190/266], Loss: 2.2706, LR: 1.00e-03\nEpoch [1/10], Batch [200/266], Loss: 0.9533, LR: 1.00e-03\nEpoch [1/10], Batch [210/266], Loss: 0.9731, LR: 1.00e-03\nEpoch [1/10], Batch [220/266], Loss: 1.2902, LR: 1.00e-03\nEpoch [1/10], Batch [230/266], Loss: 0.9895, LR: 1.00e-03\nEpoch [1/10], Batch [240/266], Loss: 1.0610, LR: 1.00e-03\nEpoch [1/10], Batch [250/266], Loss: 1.0488, LR: 1.00e-03\nEpoch [1/10], Batch [260/266], Loss: 1.0017, LR: 1.00e-03\n\nEpoch [1/10], Average Training Loss: 3.8773\nValidation Metrics:\nAUROC: 0.527\nF1: 0.667\nEpoch [2/10], Batch [10/266], Loss: 1.1294, LR: 1.00e-03\nEpoch [2/10], Batch [20/266], Loss: 1.0103, LR: 1.00e-03\nEpoch [2/10], Batch [30/266], Loss: 0.9147, LR: 1.00e-03\nEpoch [2/10], Batch [40/266], Loss: 0.9240, LR: 1.00e-03\nEpoch [2/10], Batch [50/266], Loss: 0.9294, LR: 1.00e-03\nEpoch [2/10], Batch [60/266], Loss: 1.0580, LR: 1.00e-03\nEpoch [2/10], Batch [70/266], Loss: 1.0168, LR: 1.00e-03\nEpoch [2/10], Batch [80/266], Loss: 1.0827, LR: 1.00e-03\nEpoch [2/10], Batch [90/266], Loss: 2.7507, LR: 1.00e-03\nEpoch [2/10], Batch [100/266], Loss: 1.1146, LR: 1.00e-03\nEpoch [2/10], Batch [110/266], Loss: 0.9743, LR: 1.00e-03\nEpoch [2/10], Batch [120/266], Loss: 0.9912, LR: 1.00e-03\nEpoch [2/10], Batch [130/266], Loss: 0.9156, LR: 1.00e-03\nEpoch [2/10], Batch [140/266], Loss: 1.6566, LR: 1.00e-03\nEpoch [2/10], Batch [150/266], Loss: 1.0232, LR: 1.00e-03\nEpoch [2/10], Batch [160/266], Loss: 1.0106, LR: 1.00e-03\nEpoch [2/10], Batch [170/266], Loss: 0.9533, LR: 1.00e-03\nEpoch [2/10], Batch [180/266], Loss: 1.0028, LR: 1.00e-03\nEpoch [2/10], Batch [190/266], Loss: 0.9852, LR: 1.00e-03\nEpoch [2/10], Batch [200/266], Loss: 0.9529, LR: 1.00e-03\nEpoch [2/10], Batch [210/266], Loss: 1.0203, LR: 1.00e-03\nEpoch [2/10], Batch [220/266], Loss: 1.0156, LR: 1.00e-03\nEpoch [2/10], Batch [230/266], Loss: 1.0189, LR: 1.00e-03\nEpoch [2/10], Batch [240/266], Loss: 0.8589, LR: 1.00e-03\nEpoch [2/10], Batch [250/266], Loss: 0.9944, LR: 1.00e-03\nEpoch [2/10], Batch [260/266], Loss: 1.0835, LR: 1.00e-03\n\nEpoch [2/10], Average Training Loss: 1.0329\nValidation Metrics:\nAUROC: 0.510\nF1: 0.667\nEpoch [3/10], Batch [10/266], Loss: 0.9994, LR: 1.00e-03\nEpoch [3/10], Batch [20/266], Loss: 1.3345, LR: 1.00e-03\nEpoch [3/10], Batch [30/266], Loss: 1.1536, LR: 1.00e-03\nEpoch [3/10], Batch [40/266], Loss: 1.1048, LR: 1.00e-03\nEpoch [3/10], Batch [50/266], Loss: 0.9432, LR: 1.00e-03\nEpoch [3/10], Batch [60/266], Loss: 0.9132, LR: 1.00e-03\nEpoch [3/10], Batch [70/266], Loss: 0.8636, LR: 1.00e-03\nEpoch [3/10], Batch [80/266], Loss: 0.9243, LR: 1.00e-03\nEpoch [3/10], Batch [90/266], Loss: 0.9782, LR: 1.00e-03\nEpoch [3/10], Batch [100/266], Loss: 1.1311, LR: 1.00e-03\nEpoch [3/10], Batch [110/266], Loss: 1.0215, LR: 1.00e-03\nEpoch [3/10], Batch [120/266], Loss: 1.4912, LR: 1.00e-03\nEpoch [3/10], Batch [130/266], Loss: 0.9399, LR: 1.00e-03\nEpoch [3/10], Batch [140/266], Loss: 0.8375, LR: 1.00e-03\nEpoch [3/10], Batch [150/266], Loss: 0.8574, LR: 1.00e-03\nEpoch [3/10], Batch [160/266], Loss: 1.0078, LR: 1.00e-03\nEpoch [3/10], Batch [170/266], Loss: 0.9808, LR: 1.00e-03\nEpoch [3/10], Batch [180/266], Loss: 1.4070, LR: 1.00e-03\nEpoch [3/10], Batch [190/266], Loss: 0.9572, LR: 1.00e-03\nEpoch [3/10], Batch [200/266], Loss: 0.9720, LR: 1.00e-03\nEpoch [3/10], Batch [210/266], Loss: 0.8642, LR: 1.00e-03\nEpoch [3/10], Batch [220/266], Loss: 1.0168, LR: 1.00e-03\nEpoch [3/10], Batch [230/266], Loss: 0.9195, LR: 1.00e-03\nEpoch [3/10], Batch [240/266], Loss: 0.9219, LR: 1.00e-03\nEpoch [3/10], Batch [250/266], Loss: 0.8777, LR: 1.00e-03\nEpoch [3/10], Batch [260/266], Loss: 0.9807, LR: 1.00e-03\n\nEpoch [3/10], Average Training Loss: 1.0041\nValidation Metrics:\nAUROC: 0.537\nF1: 0.667\nEpoch [4/10], Batch [10/266], Loss: 0.8279, LR: 1.00e-03\nEpoch [4/10], Batch [20/266], Loss: 0.9259, LR: 1.00e-03\nEpoch [4/10], Batch [30/266], Loss: 0.8871, LR: 1.00e-03\nEpoch [4/10], Batch [40/266], Loss: 0.9266, LR: 1.00e-03\nEpoch [4/10], Batch [50/266], Loss: 0.8674, LR: 1.00e-03\nEpoch [4/10], Batch [60/266], Loss: 1.0039, LR: 1.00e-03\nEpoch [4/10], Batch [70/266], Loss: 0.9176, LR: 1.00e-03\nEpoch [4/10], Batch [80/266], Loss: 0.8584, LR: 1.00e-03\nEpoch [4/10], Batch [90/266], Loss: 0.8263, LR: 1.00e-03\nEpoch [4/10], Batch [100/266], Loss: 0.9379, LR: 1.00e-03\nEpoch [4/10], Batch [110/266], Loss: 0.9945, LR: 1.00e-03\nEpoch [4/10], Batch [120/266], Loss: 0.7922, LR: 1.00e-03\nEpoch [4/10], Batch [130/266], Loss: 0.9087, LR: 1.00e-03\nEpoch [4/10], Batch [140/266], Loss: 0.8841, LR: 1.00e-03\nEpoch [4/10], Batch [150/266], Loss: 0.9214, LR: 1.00e-03\nEpoch [4/10], Batch [160/266], Loss: 0.8490, LR: 1.00e-03\nEpoch [4/10], Batch [170/266], Loss: 0.8393, LR: 1.00e-03\nEpoch [4/10], Batch [180/266], Loss: 0.9486, LR: 1.00e-03\nEpoch [4/10], Batch [190/266], Loss: 0.8523, LR: 1.00e-03\nEpoch [4/10], Batch [200/266], Loss: 1.1525, LR: 1.00e-03\nEpoch [4/10], Batch [210/266], Loss: 0.8523, LR: 1.00e-03\nEpoch [4/10], Batch [220/266], Loss: 0.8743, LR: 1.00e-03\nEpoch [4/10], Batch [230/266], Loss: 0.8753, LR: 1.00e-03\nEpoch [4/10], Batch [240/266], Loss: 0.9709, LR: 1.00e-03\nEpoch [4/10], Batch [250/266], Loss: 0.9322, LR: 1.00e-03\nEpoch [4/10], Batch [260/266], Loss: 0.8486, LR: 1.00e-03\n\nEpoch [4/10], Average Training Loss: 0.9042\nValidation Metrics:\nAUROC: 0.556\nF1: 0.667\nEpoch [5/10], Batch [10/266], Loss: 0.9756, LR: 1.00e-03\nEpoch [5/10], Batch [20/266], Loss: 0.8391, LR: 1.00e-03\nEpoch [5/10], Batch [30/266], Loss: 0.9553, LR: 1.00e-03\nEpoch [5/10], Batch [40/266], Loss: 0.9014, LR: 1.00e-03\nEpoch [5/10], Batch [50/266], Loss: 0.7825, LR: 1.00e-03\nEpoch [5/10], Batch [60/266], Loss: 0.9162, LR: 1.00e-03\nEpoch [5/10], Batch [70/266], Loss: 1.0400, LR: 1.00e-03\nEpoch [5/10], Batch [80/266], Loss: 0.9117, LR: 1.00e-03\nEpoch [5/10], Batch [90/266], Loss: 0.9438, LR: 1.00e-03\nEpoch [5/10], Batch [100/266], Loss: 1.0395, LR: 1.00e-03\nEpoch [5/10], Batch [110/266], Loss: 0.8477, LR: 1.00e-03\nEpoch [5/10], Batch [120/266], Loss: 0.7945, LR: 1.00e-03\nEpoch [5/10], Batch [130/266], Loss: 0.9616, LR: 1.00e-03\nEpoch [5/10], Batch [140/266], Loss: 0.8752, LR: 1.00e-03\nEpoch [5/10], Batch [150/266], Loss: 0.8632, LR: 1.00e-03\nEpoch [5/10], Batch [160/266], Loss: 0.7301, LR: 1.00e-03\nEpoch [5/10], Batch [170/266], Loss: 0.8067, LR: 1.00e-03\nEpoch [5/10], Batch [180/266], Loss: 0.9066, LR: 1.00e-03\nEpoch [5/10], Batch [190/266], Loss: 0.8157, LR: 1.00e-03\nEpoch [5/10], Batch [200/266], Loss: 0.9055, LR: 1.00e-03\nEpoch [5/10], Batch [210/266], Loss: 2.2439, LR: 1.00e-03\nEpoch [5/10], Batch [220/266], Loss: 0.8597, LR: 1.00e-03\nEpoch [5/10], Batch [230/266], Loss: 1.2627, LR: 1.00e-03\nEpoch [5/10], Batch [240/266], Loss: 0.7718, LR: 1.00e-03\nEpoch [5/10], Batch [250/266], Loss: 0.9598, LR: 1.00e-03\nEpoch [5/10], Batch [260/266], Loss: 0.8858, LR: 1.00e-03\n\nEpoch [5/10], Average Training Loss: 0.9085\nValidation Metrics:\nAUROC: 0.552\nF1: 0.667\nEpoch [6/10], Batch [10/266], Loss: 0.8029, LR: 1.00e-03\nEpoch [6/10], Batch [20/266], Loss: 0.8783, LR: 1.00e-03\nEpoch [6/10], Batch [30/266], Loss: 0.9159, LR: 1.00e-03\nEpoch [6/10], Batch [40/266], Loss: 0.7392, LR: 1.00e-03\nEpoch [6/10], Batch [50/266], Loss: 0.7015, LR: 1.00e-03\nEpoch [6/10], Batch [60/266], Loss: 0.7880, LR: 1.00e-03\nEpoch [6/10], Batch [70/266], Loss: 0.8806, LR: 1.00e-03\nEpoch [6/10], Batch [80/266], Loss: 0.9317, LR: 1.00e-03\nEpoch [6/10], Batch [90/266], Loss: 0.7701, LR: 1.00e-03\nEpoch [6/10], Batch [100/266], Loss: 0.8716, LR: 1.00e-03\nEpoch [6/10], Batch [110/266], Loss: 0.8281, LR: 1.00e-03\nEpoch [6/10], Batch [120/266], Loss: 0.7638, LR: 1.00e-03\nEpoch [6/10], Batch [130/266], Loss: 0.9149, LR: 1.00e-03\nEpoch [6/10], Batch [140/266], Loss: 0.9510, LR: 1.00e-03\nEpoch [6/10], Batch [150/266], Loss: 0.9690, LR: 1.00e-03\nEpoch [6/10], Batch [160/266], Loss: 1.9287, LR: 1.00e-03\nEpoch [6/10], Batch [170/266], Loss: 0.9490, LR: 1.00e-03\nEpoch [6/10], Batch [180/266], Loss: 0.7809, LR: 1.00e-03\nEpoch [6/10], Batch [190/266], Loss: 0.8637, LR: 1.00e-03\nEpoch [6/10], Batch [200/266], Loss: 0.9526, LR: 1.00e-03\nEpoch [6/10], Batch [210/266], Loss: 0.8703, LR: 1.00e-03\nEpoch [6/10], Batch [220/266], Loss: 0.8127, LR: 1.00e-03\nEpoch [6/10], Batch [230/266], Loss: 0.9011, LR: 1.00e-03\nEpoch [6/10], Batch [240/266], Loss: 0.7795, LR: 1.00e-03\nEpoch [6/10], Batch [250/266], Loss: 0.9518, LR: 1.00e-03\nEpoch [6/10], Batch [260/266], Loss: 0.8203, LR: 1.00e-03\n\nEpoch [6/10], Average Training Loss: 0.8914\nValidation Metrics:\nAUROC: 0.567\nF1: 0.667\nEpoch [7/10], Batch [10/266], Loss: 0.8933, LR: 1.00e-03\nEpoch [7/10], Batch [20/266], Loss: 0.8999, LR: 1.00e-03\nEpoch [7/10], Batch [30/266], Loss: 0.8801, LR: 1.00e-03\nEpoch [7/10], Batch [40/266], Loss: 0.8223, LR: 1.00e-03\nEpoch [7/10], Batch [50/266], Loss: 0.8750, LR: 1.00e-03\nEpoch [7/10], Batch [60/266], Loss: 0.9186, LR: 1.00e-03\nEpoch [7/10], Batch [70/266], Loss: 0.8879, LR: 1.00e-03\nEpoch [7/10], Batch [80/266], Loss: 0.9033, LR: 1.00e-03\nEpoch [7/10], Batch [90/266], Loss: 0.7750, LR: 1.00e-03\nEpoch [7/10], Batch [100/266], Loss: 0.9122, LR: 1.00e-03\nEpoch [7/10], Batch [110/266], Loss: 0.9562, LR: 1.00e-03\nEpoch [7/10], Batch [120/266], Loss: 1.0012, LR: 1.00e-03\nEpoch [7/10], Batch [130/266], Loss: 0.9799, LR: 1.00e-03\nEpoch [7/10], Batch [140/266], Loss: 0.7493, LR: 1.00e-03\nEpoch [7/10], Batch [150/266], Loss: 0.8236, LR: 1.00e-03\nEpoch [7/10], Batch [160/266], Loss: 0.8129, LR: 1.00e-03\nEpoch [7/10], Batch [170/266], Loss: 0.9207, LR: 1.00e-03\nEpoch [7/10], Batch [180/266], Loss: 0.8392, LR: 1.00e-03\nEpoch [7/10], Batch [190/266], Loss: 0.8330, LR: 1.00e-03\nEpoch [7/10], Batch [200/266], Loss: 0.8408, LR: 1.00e-03\nEpoch [7/10], Batch [210/266], Loss: 0.7458, LR: 1.00e-03\nEpoch [7/10], Batch [220/266], Loss: 0.9539, LR: 1.00e-03\nEpoch [7/10], Batch [230/266], Loss: 0.7882, LR: 1.00e-03\nEpoch [7/10], Batch [240/266], Loss: 0.7390, LR: 1.00e-03\nEpoch [7/10], Batch [250/266], Loss: 0.8389, LR: 1.00e-03\nEpoch [7/10], Batch [260/266], Loss: 1.7768, LR: 1.00e-03\n\nEpoch [7/10], Average Training Loss: 0.8808\nValidation Metrics:\nAUROC: 0.578\nF1: 0.667\nEpoch [8/10], Batch [10/266], Loss: 0.9609, LR: 1.00e-03\nEpoch [8/10], Batch [20/266], Loss: 0.9337, LR: 1.00e-03\nEpoch [8/10], Batch [30/266], Loss: 0.9855, LR: 1.00e-03\nEpoch [8/10], Batch [40/266], Loss: 0.9588, LR: 1.00e-03\nEpoch [8/10], Batch [50/266], Loss: 0.8441, LR: 1.00e-03\nEpoch [8/10], Batch [60/266], Loss: 0.9477, LR: 1.00e-03\nEpoch [8/10], Batch [70/266], Loss: 0.9057, LR: 1.00e-03\nEpoch [8/10], Batch [80/266], Loss: 0.9500, LR: 1.00e-03\nEpoch [8/10], Batch [90/266], Loss: 0.9364, LR: 1.00e-03\nEpoch [8/10], Batch [100/266], Loss: 0.9342, LR: 1.00e-03\nEpoch [8/10], Batch [110/266], Loss: 0.9839, LR: 1.00e-03\nEpoch [8/10], Batch [120/266], Loss: 0.8012, LR: 1.00e-03\nEpoch [8/10], Batch [130/266], Loss: 0.8743, LR: 1.00e-03\nEpoch [8/10], Batch [140/266], Loss: 0.8065, LR: 1.00e-03\nEpoch [8/10], Batch [150/266], Loss: 0.7655, LR: 1.00e-03\nEpoch [8/10], Batch [160/266], Loss: 0.9037, LR: 1.00e-03\nEpoch [8/10], Batch [170/266], Loss: 0.8361, LR: 1.00e-03\nEpoch [8/10], Batch [180/266], Loss: 1.1358, LR: 1.00e-03\nEpoch [8/10], Batch [190/266], Loss: 0.8380, LR: 1.00e-03\nEpoch [8/10], Batch [200/266], Loss: 0.8424, LR: 1.00e-03\nEpoch [8/10], Batch [210/266], Loss: 0.7410, LR: 1.00e-03\nEpoch [8/10], Batch [220/266], Loss: 0.9451, LR: 1.00e-03\nEpoch [8/10], Batch [230/266], Loss: 0.8762, LR: 1.00e-03\nEpoch [8/10], Batch [240/266], Loss: 0.8214, LR: 1.00e-03\nEpoch [8/10], Batch [250/266], Loss: 0.9805, LR: 1.00e-03\nEpoch [8/10], Batch [260/266], Loss: 0.7371, LR: 1.00e-03\n\nEpoch [8/10], Average Training Loss: 0.9068\nValidation Metrics:\nAUROC: 0.576\nF1: 0.583\nEpoch [9/10], Batch [10/266], Loss: 0.7576, LR: 1.00e-03\nEpoch [9/10], Batch [20/266], Loss: 0.7293, LR: 1.00e-03\nEpoch [9/10], Batch [30/266], Loss: 0.7590, LR: 1.00e-03\nEpoch [9/10], Batch [40/266], Loss: 0.9649, LR: 1.00e-03\nEpoch [9/10], Batch [50/266], Loss: 0.6365, LR: 1.00e-03\nEpoch [9/10], Batch [60/266], Loss: 0.8490, LR: 1.00e-03\nEpoch [9/10], Batch [70/266], Loss: 0.8107, LR: 1.00e-03\nEpoch [9/10], Batch [80/266], Loss: 0.8333, LR: 1.00e-03\nEpoch [9/10], Batch [90/266], Loss: 0.7363, LR: 1.00e-03\nEpoch [9/10], Batch [100/266], Loss: 0.7067, LR: 1.00e-03\nEpoch [9/10], Batch [110/266], Loss: 0.8881, LR: 1.00e-03\nEpoch [9/10], Batch [120/266], Loss: 0.7277, LR: 1.00e-03\nEpoch [9/10], Batch [130/266], Loss: 0.8229, LR: 1.00e-03\nEpoch [9/10], Batch [140/266], Loss: 1.1670, LR: 1.00e-03\nEpoch [9/10], Batch [150/266], Loss: 1.0529, LR: 1.00e-03\nEpoch [9/10], Batch [160/266], Loss: 0.7927, LR: 1.00e-03\nEpoch [9/10], Batch [170/266], Loss: 0.7960, LR: 1.00e-03\nEpoch [9/10], Batch [180/266], Loss: 0.6559, LR: 1.00e-03\nEpoch [9/10], Batch [190/266], Loss: 0.6821, LR: 1.00e-03\nEpoch [9/10], Batch [200/266], Loss: 0.7499, LR: 1.00e-03\nEpoch [9/10], Batch [210/266], Loss: 0.9876, LR: 1.00e-03\nEpoch [9/10], Batch [220/266], Loss: 0.9596, LR: 1.00e-03\nEpoch [9/10], Batch [230/266], Loss: 0.6660, LR: 1.00e-03\nEpoch [9/10], Batch [240/266], Loss: 0.6833, LR: 1.00e-03\nEpoch [9/10], Batch [250/266], Loss: 0.8752, LR: 1.00e-03\nEpoch [9/10], Batch [260/266], Loss: 0.6835, LR: 1.00e-03\n\nEpoch [9/10], Average Training Loss: 0.9522\nValidation Metrics:\nAUROC: 0.570\nF1: 0.628\nEpoch [10/10], Batch [10/266], Loss: 0.7581, LR: 1.00e-03\nEpoch [10/10], Batch [20/266], Loss: 0.7635, LR: 1.00e-03\nEpoch [10/10], Batch [30/266], Loss: 0.6043, LR: 1.00e-03\nEpoch [10/10], Batch [40/266], Loss: 0.9261, LR: 1.00e-03\nEpoch [10/10], Batch [50/266], Loss: 0.8376, LR: 1.00e-03\nEpoch [10/10], Batch [60/266], Loss: 0.7973, LR: 1.00e-03\nEpoch [10/10], Batch [70/266], Loss: 0.7038, LR: 1.00e-03\nEpoch [10/10], Batch [80/266], Loss: 0.6279, LR: 1.00e-03\nEpoch [10/10], Batch [90/266], Loss: 0.6942, LR: 1.00e-03\nEpoch [10/10], Batch [100/266], Loss: 0.9508, LR: 1.00e-03\nEpoch [10/10], Batch [110/266], Loss: 1.0808, LR: 1.00e-03\nEpoch [10/10], Batch [120/266], Loss: 0.7972, LR: 1.00e-03\nEpoch [10/10], Batch [130/266], Loss: 0.8875, LR: 1.00e-03\nEpoch [10/10], Batch [140/266], Loss: 0.9353, LR: 1.00e-03\nEpoch [10/10], Batch [150/266], Loss: 0.8465, LR: 1.00e-03\nEpoch [10/10], Batch [160/266], Loss: 0.7338, LR: 1.00e-03\nEpoch [10/10], Batch [170/266], Loss: 0.8364, LR: 1.00e-03\nEpoch [10/10], Batch [180/266], Loss: 0.5458, LR: 1.00e-03\nEpoch [10/10], Batch [190/266], Loss: 0.9561, LR: 1.00e-03\nEpoch [10/10], Batch [200/266], Loss: 0.8178, LR: 1.00e-03\nEpoch [10/10], Batch [210/266], Loss: 0.8629, LR: 1.00e-03\nEpoch [10/10], Batch [220/266], Loss: 0.5879, LR: 1.00e-03\nEpoch [10/10], Batch [230/266], Loss: 0.8685, LR: 1.00e-03\nEpoch [10/10], Batch [240/266], Loss: 0.9263, LR: 1.00e-03\nEpoch [10/10], Batch [250/266], Loss: 0.7028, LR: 1.00e-03\nEpoch [10/10], Batch [260/266], Loss: 0.6131, LR: 1.00e-03\n\nEpoch [10/10], Average Training Loss: 0.7670\nValidation Metrics:\nAUROC: 0.567\nF1: 0.639\n\nTraining completed! Loading best model for final evaluation...\nSuccessfully loaded best model.\nError loading best model: invalid literal for int() with base 10: 'final'\nAnalysis report generated at /kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/model_analysis.md\n\nEvaluation complete! Check the log directory for visualizations and analysis.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!zip -r output.zip /kaggle/working/runs/early_fusion_20250519_091941_20250519_091941","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:44:00.247209Z","iopub.execute_input":"2025-05-19T09:44:00.247984Z","iopub.status.idle":"2025-05-19T09:44:49.577095Z","shell.execute_reply.started":"2025-05-19T09:44:00.247948Z","shell.execute_reply":"2025-05-19T09:44:49.576281Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/ (stored 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_2.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_7.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_8.png (deflated 19%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_5.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_incorrect_examples_epoch_8.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_9.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_5.png (deflated 20%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_correct_examples_epoch_8.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_6.png (deflated 20%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_5.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_correct_examples_epoch_6.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_8.png (deflated 10%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_6.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/events.out.tfevents.1747646381.28491ac94b58.35.21 (deflated 2%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_correct_examples_epoch_0.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_8.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_1.png (deflated 19%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_incorrect_examples_epoch_4.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_4.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_metrics.csv (deflated 58%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_1.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_correct_examples_epoch_4.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/model_analysis.md (deflated 54%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_7.png (deflated 19%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_correct_examples_epoch_2.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_7.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_2.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_incorrect_examples_epoch_6.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_6.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_9.png (deflated 19%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_incorrect_examples_epoch_2.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/best_model_f1.pth (deflated 7%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_4.png (deflated 19%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/best_model_auroc.pth (deflated 7%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_2.png (deflated 20%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_1.png (deflated 10%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_0.png (deflated 10%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_0.png (deflated 20%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_9.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_incorrect_examples_epoch_0.png (deflated 90%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_0.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_samples_epoch_3.png (deflated 0%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_4.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_roc_curve_epoch_3.png (deflated 11%)\n  adding: kaggle/working/runs/early_fusion_20250519_091941_20250519_091941/val_confusion_matrix_epoch_3.png (deflated 20%)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:19:29.084859Z","iopub.execute_input":"2025-05-19T09:19:29.085645Z","iopub.status.idle":"2025-05-19T09:19:29.665341Z","shell.execute_reply.started":"2025-05-19T09:19:29.085622Z","shell.execute_reply":"2025-05-19T09:19:29.664384Z"}},"outputs":[],"execution_count":27}]}